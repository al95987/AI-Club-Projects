{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6120cb39",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing and Text Mining\n",
    "\n",
    "Welcome to this workshop! In this session, we will cover the basics of Natural Language Processing (NLP) and text mining. We'll explore some essential techniques such as tokenization, part-of-speech tagging, sentiment analysis, and more.\n",
    "\n",
    "We'll also learn how to download tweets, create word clouds, and pre-process data.\n",
    "\n",
    "Let's get started!\n",
    "\n",
    "# 1: Importing Necessary Libraries\n",
    "\n",
    "In this cell, we'll import the necessary libraries to handle data processing and text cleaning. These include pandas for data manipulation, NLTK for natural language processing, and contractions for handling contractions in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12491f26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting contractions\n",
      "  Downloading contractions-0.1.73-py2.py3-none-any.whl (8.7 kB)\n",
      "Collecting textsearch>=0.0.21\n",
      "  Downloading textsearch-0.0.24-py2.py3-none-any.whl (7.6 kB)\n",
      "Collecting pyahocorasick\n",
      "  Downloading pyahocorasick-2.0.0-cp310-cp310-macosx_10_9_x86_64.whl (37 kB)\n",
      "Collecting anyascii\n",
      "  Downloading anyascii-0.3.2-py3-none-any.whl (289 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.9/289.9 kB\u001b[0m \u001b[31m71.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
      "Successfully installed anyascii-0.3.2 contractions-0.1.73 pyahocorasick-2.0.0 textsearch-0.0.24\n",
      "Collecting tweety\n",
      "  Using cached tweety-0.1.6.tar.gz (4.8 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in ./anaconda3/lib/python3.10/site-packages (from tweety) (4.11.1)\n",
      "Requirement already satisfied: pandas in ./anaconda3/lib/python3.10/site-packages (from tweety) (1.5.3)\n",
      "Collecting selenium\n",
      "  Downloading selenium-4.11.2-py3-none-any.whl (7.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m120.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting spacy\n",
      "  Downloading spacy-3.6.0-cp310-cp310-macosx_10_9_x86_64.whl (6.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m86.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:03\u001b[0mm\n",
      "\u001b[?25hCollecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.0/126.0 kB\u001b[0m \u001b[31m159.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m636.8/636.8 kB\u001b[0m \u001b[31m86.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting chromedriver-binary==74.0.3729.6.0\n",
      "  Downloading chromedriver-binary-74.0.3729.6.0.tar.gz (3.3 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: soupsieve>1.2 in ./anaconda3/lib/python3.10/site-packages (from beautifulsoup4->tweety) (2.3.2.post1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in ./anaconda3/lib/python3.10/site-packages (from pandas->tweety) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./anaconda3/lib/python3.10/site-packages (from pandas->tweety) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.21.0 in ./anaconda3/lib/python3.10/site-packages (from pandas->tweety) (1.23.5)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in ./anaconda3/lib/python3.10/site-packages (from selenium->tweety) (2023.5.7)\n",
      "Collecting trio-websocket~=0.9\n",
      "  Downloading trio_websocket-0.10.3-py3-none-any.whl (17 kB)\n",
      "Collecting trio~=0.17\n",
      "  Downloading trio-0.22.2-py3-none-any.whl (400 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.2/400.2 kB\u001b[0m \u001b[31m88.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: urllib3[socks]<3,>=1.26 in ./anaconda3/lib/python3.10/site-packages (from selenium->tweety) (1.26.14)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (5.2.1)\n",
      "Requirement already satisfied: setuptools in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (65.6.3)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4\n",
      "  Downloading pydantic-1.10.12-cp310-cp310-macosx_10_9_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m63.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hCollecting thinc<8.2.0,>=8.1.8\n",
      "  Downloading thinc-8.1.10-cp310-cp310-macosx_10_9_x86_64.whl (859 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.1/859.1 kB\u001b[0m \u001b[31m75.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Downloading spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (2.31.0)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Downloading wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.9-cp310-cp310-macosx_10_9_x86_64.whl (18 kB)\n",
      "Requirement already satisfied: jinja2 in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (3.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (22.0)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.8-cp310-cp310-macosx_10_9_x86_64.whl (106 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m107.0/107.0 kB\u001b[0m \u001b[31m75.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.38.0 in ./anaconda3/lib/python3.10/site-packages (from spacy->tweety) (4.64.1)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Downloading typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.9/45.9 kB\u001b[0m \u001b[31m73.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.7-cp310-cp310-macosx_10_9_x86_64.whl (32 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 kB\u001b[0m \u001b[31m74.2 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting pathy>=0.10.0\n",
      "  Downloading pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m109.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp310-cp310-macosx_10_9_x86_64.whl (492 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m492.7/492.7 kB\u001b[0m \u001b[31m104.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in ./anaconda3/lib/python3.10/site-packages (from textblob->tweety) (3.7)\n",
      "Requirement already satisfied: joblib in ./anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob->tweety) (1.1.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob->tweety) (2022.7.9)\n",
      "Requirement already satisfied: click in ./anaconda3/lib/python3.10/site-packages (from nltk>=3.1->textblob->tweety) (8.0.4)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in ./anaconda3/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy->tweety) (4.4.0)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->tweety) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->tweety) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./anaconda3/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy->tweety) (3.4)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp310-cp310-macosx_10_9_x86_64.whl (6.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.1/6.1 MB\u001b[0m \u001b[31m100.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:02\u001b[0mm\n",
      "\u001b[?25hCollecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: sniffio in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium->tweety) (1.2.0)\n",
      "Requirement already satisfied: sortedcontainers in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium->tweety) (2.4.0)\n",
      "Collecting exceptiongroup>=1.0.0rc9\n",
      "  Downloading exceptiongroup-1.1.2-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: attrs>=20.1.0 in ./anaconda3/lib/python3.10/site-packages (from trio~=0.17->selenium->tweety) (22.1.0)\n",
      "Collecting outcome\n",
      "  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting wsproto>=0.14\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in ./anaconda3/lib/python3.10/site-packages (from urllib3[socks]<3,>=1.26->selenium->tweety) (1.7.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./anaconda3/lib/python3.10/site-packages (from jinja2->spacy->tweety) (2.1.1)\n",
      "Collecting h11<1,>=0.9.0\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m122.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: tweety, chromedriver-binary\n",
      "  Building wheel for tweety (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for tweety: filename=tweety-0.1.6-py3-none-any.whl size=4913 sha256=7610c060c20d24bf8666aa58dcf51078a996e1f822a75ac87a803bcec2fb8f59\n",
      "  Stored in directory: /Users/Amanda/Library/Caches/pip/wheels/e2/e8/ab/0858de7a96dc6075dfca01fee9127ba9a9ebfa1110b3cfad62\n",
      "  Building wheel for chromedriver-binary (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for chromedriver-binary: filename=chromedriver_binary-74.0.3729.6.0-py3-none-any.whl size=7017020 sha256=2240f859b3ac5b9aad103f38205f0ddf6e5cfbd7556c74b578d5522af1a0f37a\n",
      "  Stored in directory: /Users/Amanda/Library/Caches/pip/wheels/b1/f4/62/6f2b194253dbb4f0d926c9dc604b58eb228b55489d8bf6ee1d\n",
      "Successfully built tweety chromedriver-binary\n",
      "Installing collected packages: cymem, chromedriver-binary, wasabi, typer, spacy-loggers, spacy-legacy, pydantic, outcome, murmurhash, langcodes, h11, exceptiongroup, catalogue, blis, wsproto, vaderSentiment, trio, textblob, srsly, preshed, pathy, trio-websocket, confection, thinc, selenium, spacy, tweety\n",
      "Successfully installed blis-0.7.10 catalogue-2.0.9 chromedriver-binary-74.0.3729.6.0 confection-0.1.0 cymem-2.0.7 exceptiongroup-1.1.2 h11-0.14.0 langcodes-3.3.0 murmurhash-1.0.9 outcome-1.2.0 pathy-0.10.2 preshed-3.0.8 pydantic-1.10.12 selenium-4.11.2 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 textblob-0.17.1 thinc-8.1.10 trio-0.22.2 trio-websocket-0.10.3 tweety-0.1.6 typer-0.9.0 vaderSentiment-3.3.2 wasabi-1.1.2 wsproto-1.2.0\n",
      "Collecting wordcloud\n",
      "  Using cached wordcloud-1.9.2-cp310-cp310-macosx_10_9_x86_64.whl (160 kB)\n",
      "Requirement already satisfied: numpy>=1.6.1 in ./anaconda3/lib/python3.10/site-packages (from wordcloud) (1.23.5)\n",
      "Requirement already satisfied: pillow in ./anaconda3/lib/python3.10/site-packages (from wordcloud) (9.4.0)\n",
      "Requirement already satisfied: matplotlib in ./anaconda3/lib/python3.10/site-packages (from wordcloud) (3.7.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (4.25.0)\n",
      "Requirement already satisfied: packaging>=20.0 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (22.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.4.4)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (3.0.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (2.8.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./anaconda3/lib/python3.10/site-packages (from matplotlib->wordcloud) (1.0.5)\n",
      "Requirement already satisfied: six>=1.5 in ./anaconda3/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib->wordcloud) (1.16.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.9.2\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m77.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in ./anaconda3/lib/python3.10/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=6587bb61af98794f72e3c26abed98a9b4efbedcf72547df8a9edfdced6e81faa\n",
      "  Stored in directory: /Users/Amanda/Library/Caches/pip/wheels/c4/16/af/1889804d8b7c0c041cadee8e29673a938a332acbf2865c70a1\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n",
      "Collecting emoji\n",
      "  Downloading emoji-2.7.0.tar.gz (361 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m361.8/361.8 kB\u001b[0m \u001b[31m295.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: emoji\n",
      "  Building wheel for emoji (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.7.0-py2.py3-none-any.whl size=356563 sha256=3e2a1c5ec5f1f776a65d43032224803d7a9441ac8a77c8d8b3b20666b9c5467a\n",
      "  Stored in directory: /Users/Amanda/Library/Caches/pip/wheels/42/95/47/e23c9fc8d00c65cce4ba32e4f6c5b08d018603e2d4d82db7a9\n",
      "Successfully built emoji\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.7.0\n"
     ]
    }
   ],
   "source": [
    "!pip install contractions\n",
    "!pip install tweety\n",
    "!pip install wordcloud\n",
    "!pip install langdetect\n",
    "!pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90fb1572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/Amanda/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/Amanda/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/Amanda/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /Users/Amanda/nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/Amanda/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3736a521",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tweepy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m wordnet\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtweepy\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwordcloud\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordCloud\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tweepy'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import contractions\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tweepy\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8588ad3a",
   "metadata": {},
   "source": [
    "# Downloading Tweets\n",
    "\n",
    "You can use libraries like Tweepy to download tweets for analysis. Make sure to create a Twitter Developer Account and obtain API keys.\n",
    "\n",
    "Here's an example code snippet to get you started:\n",
    "\n",
    "Follow the steps below to create a Twitter Developer Account and obtain the necessary API keys:\n",
    "\n",
    "\n",
    "## Step 1: Apply for a Developer Account\n",
    "\n",
    "1. Go to Twitter Developer's Portal.\n",
    "2. Click on \"Apply for a developer account.\"\n",
    "3. Follow the prompts to log in with your Twitter account or create a new account.\n",
    "4. Select the reason for wanting to access the Twitter API (such as \"Academic Research\" or \"Personal Use\").\n",
    "\n",
    "## Step 2: Complete the Application\n",
    "\n",
    "1. Fill out the application form with detailed information about how you plan to use the Twitter API.\n",
    "2. Explain your intended use case, the methods you'll employ, and any other information requested.\n",
    "3. Read and accept the Developer Agreement.\n",
    "4. Submit the application.\n",
    "\n",
    "**Note:** Approval might take some time, depending on the details provided.\n",
    "\n",
    "## Step 3: Create a Project and Developer App\n",
    "\n",
    "1. Once approved, go to the Twitter Developer Dashboard.\n",
    "2. Click on \"Create Project.\"\n",
    "3. Name your project and provide a brief description.\n",
    "4. Inside the project, click on \"Create App.\"\n",
    "5. Name your app and link it to your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc2b1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up API credentials\n",
    "\n",
    "consumer_key = 'your_consumer_key'\n",
    "consumer_secret = 'your_consumer_secret'\n",
    "access_token = 'your_access_token'\n",
    "access_token_secret = 'your_access_token_secret'\n",
    "\n",
    "# Authenticate and access Twitter API\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "# Search for tweets\n",
    "\n",
    "tweets = api.search(q='NLP', count=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5430bf06",
   "metadata": {},
   "source": [
    "## Step 4: Obtain API Keys and Tokens\n",
    "\n",
    "1. Inside your app's details page, you'll find the \"Keys and Tokens\" tab.\n",
    "2. Here, you will see your \"API Key,\" \"API Key Secret,\" \"Access Token,\" and \"Access Token Secret.\"\n",
    "3. Copy these credentials, as you will need them to authenticate and access the Twitter API from your code.\n",
    "\n",
    "## Important Considerations:\n",
    "\n",
    "- Keep your keys and tokens private and secure.\n",
    "- Respect Twitter's Developer Agreement and use the API in accordance with the rules and guidelines.\n",
    "- Monitor your usage to stay within the API rate limits.\n",
    "\n",
    "With your Twitter Developer Account set up, you are now ready to explore and analyze Twitter data using various tools and libraries. Happy coding!\n",
    "\n",
    "# 2: Reading Data\n",
    "Here we are reading the 'Tweets.csv' file into a DataFrame. We're using the 'utf-8' encoding to avoid issues with special characters.\n",
    "\n",
    "The tweets I collected were on Apple's iPhone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab41bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/Users/Amanda/Downloads/Tweets.csv', encoding='utf-8') as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcfb068",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c29cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a1db1e",
   "metadata": {},
   "source": [
    "By specifying the encoding as 'utf-8', you're informing the system to interpret the bytes in the file using the UTF-8 encoding, which usually handles special characters well.\n",
    "\n",
    "Make sure that the CSV file is actually encoded in UTF-8. If you still encounter issues, you may want to open the CSV file in a text editor that allows you to view and change the encoding (such as Notepad++), and make sure it's saved in UTF-8 format.\n",
    "\n",
    "# 3: Cleaning DataFrame\n",
    "\n",
    "In this step, we drop unnecessary columns from the DataFrame to simplify our data structure.\n",
    "\n",
    "# 4: Handling Missing Values\n",
    "\n",
    "We'll now check for and drop any rows with missing values in the 'Text', 'Followers', or 'Friend Count' columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['text','selected_text', 'sentiment'])\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05dfa0b",
   "metadata": {},
   "source": [
    "# 5: Text Preprocessing\n",
    "\n",
    "In the following cells, we preprocess the text data through several steps. We'll apply contractions, tokenize the text, convert it to lowercase, remove punctuation and stop words, apply part-of-speech tagging, and lemmatize the words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b167eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining functions in the beginning\n",
    "\n",
    "# Define a function to remove special characters from a tokenized text for later use\n",
    "def remove_special_characters(tokenized_text):\n",
    "    return [re.sub(r'[^\\w\\s]', '', word) for word in tokenized_text]\n",
    "\n",
    "def is_valid_word(word):\n",
    "    return len(word) > 1 and word.isalpha()\n",
    "\n",
    "def filter_english_sentences(text):\n",
    "    english_text = []\n",
    "    for sentence in text.split('.'):\n",
    "        try:\n",
    "            if detect(sentence) == 'en':\n",
    "                english_text.append(sentence)\n",
    "        except LangDetectException:\n",
    "            pass\n",
    "    return ' '.join(english_text)\n",
    "\n",
    "\n",
    "# Function to get WordNet POS tag\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636d5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws = df[['text','selected_text', 'sentiment']]\n",
    "rws['no_contract'] = rws['text'].apply(lambda x: [contractions.fix(word) for word in x.split()])\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808d869f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the text\n",
    "rws['tokenized'] = rws['no_contract'].apply(lambda x: word_tokenize(' '.join(x)))\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b92c766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to lowercase\n",
    "rws['lower'] = rws['tokenized'].apply(lambda x: [word.lower() for word in x])\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d5918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "punc = string.punctuation\n",
    "rws['no_punc'] = rws['lower'].apply(lambda x: [word for word in x if word not in punc])\n",
    "\n",
    "rws.head()\n",
    "te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beaaa41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "stop_words = set(stopwords.words('english'))\n",
    "rws['stopwords_removed'] = rws['no_punc'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdfe57",
   "metadata": {},
   "source": [
    "Customizing stopwords can be a valuable step in natural language processing (NLP) to enhance the meaningful content in a given text analysis. Stopwords are common words that are often removed because they contribute little to the overall meaning, especially in tasks like text mining, topic modeling, and visualization (like word clouds).\n",
    "\n",
    "However, in some cases, the standard stopword list might remove words that are important for a particular analysis, or it might not remove words that are unimportant in a specific context. Customizing the stopword list can help to fine-tune the analysis to a specific use case.\n",
    "\n",
    "Here's how you could customize the stopwords for your word cloud, along with an explanation of the logic:\n",
    "\n",
    "**1.Start with the Standard Stopwords:** Most NLP libraries, like NLTK, provide a standard list of stopwords. You can start with this list.\n",
    "\n",
    "**2.Add Custom Stopwords:** Depending on the context of your analysis, you might have specific words that are common but uninformative. You can add these to the stopword list.\n",
    "\n",
    "**3.Remove Necessary Stopwords:** If the standard list removes words that are meaningful in your context, you can remove these from the stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c58d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom stopwords that are specific to your context\n",
    "custom_stopwords = ['https', 'apple', 'iphone']\n",
    "stop_words.update(custom_stopwords)\n",
    "\n",
    "# If necessary, remove stopwords that are meaningful in your context\n",
    "stop_words.discard('not')\n",
    "\n",
    "# Function to remove stopwords from a tokenized text\n",
    "def remove_stopwords(tokenized_text):\n",
    "    return [word for word in tokenized_text if word.lower() not in stop_words]\n",
    "\n",
    "# Remove stopwords\n",
    "rws['stopwords_removed'] = rws['tokenized'].apply(remove_stopwords)\n",
    "\n",
    "# Remove special characters\n",
    "rws['filtered'] = rws['stopwords_removed'].apply(remove_special_characters)\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c71c46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38674a6f",
   "metadata": {},
   "source": [
    "### Creating a Word Cloud\n",
    "\n",
    "A Word Cloud (or Tag Cloud) is a visual representation of text data. It displays a list of words, the importance of each being shown with font size or color. This format is useful for quickly perceiving the most prominent terms in a large collection of text, and it's often used to visualize free-form text like social media posts, survey responses, and more.\n",
    "\n",
    "In the context of Natural Language Processing (NLP) and text mining, a Word Cloud can provide insights into the frequency or significance of words within a corpus of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904759a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all the tokenized words into one string\n",
    "wordcloud_text = ' '.join([' '.join(tokens) for tokens in rws['stopwords_removed']])\n",
    "\n",
    "# Creating a word cloud\n",
    "wordcloud = WordCloud(background_color='white').generate(wordcloud_text)\n",
    "\n",
    "# Displaying the word cloud\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4b48a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply POS tagging\n",
    "rws['pos_tags'] = rws['stopwords_removed'].apply(nltk.tag.pos_tag)\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b066d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply WordNet POS tag\n",
    "rws['wordnet_pos'] = rws['pos_tags'].apply(lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n",
    "\n",
    "rws.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d28fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize words\n",
    "wnl = WordNetLemmatizer()\n",
    "rws['lemmatized'] = rws['wordnet_pos'].apply(lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n",
    "\n",
    "rws.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66007348",
   "metadata": {},
   "source": [
    "# 6: Saving the Cleaned Data\n",
    "\n",
    "We'll save the cleaned and preprocessed data to a new CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd282d",
   "metadata": {},
   "outputs": [],
   "source": [
    "rws.to_csv('indeed_scrape_clean.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
